{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from data.dataset import TextResponseDataset\n",
    "import pandas as pd\n",
    "import os\n",
    "import itertools as it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {'mse':0, 'auc':1, 'll':2, 'acc':3, 'perp':4, 'npmi':5, 'shuffle':6}\n",
    "method_names = {'hstm-all':'HSTM', 'bert':'BERT', 'stm':'STM', 'stm+bow':'HSTM-Het', 'slda':'sLDA'}\n",
    "out = os.path.join('..', 'out')\n",
    "lda_pretrained = {'hstm-all', 'stm', 'stm+bow'}\n",
    "\n",
    "models = ['hstm-all', 'stm', 'stm+bow', 'slda', 'bert']\n",
    "\n",
    "datasets = ['amazon', \n",
    "            'amazon_binary', \n",
    "            'peerread', \n",
    "            'yelp', \n",
    "            'immigration', \n",
    "            'samesex', \n",
    "            'deathpenalty', \n",
    "            'guncontrol']\n",
    "\n",
    "framing_corpus_topics = {'immigration', 'samesex', 'deathpenalty', 'guncontrol'}\n",
    "\n",
    "Cs = [1e-4, 5e-5, 1e-5, 5e-6, 1e-6]\n",
    "Ctopics = [1e-4, 5e-5, 1e-5, 5e-6, 1e-6]\n",
    "num_folds = 5\n",
    "n_metrics = len(metrics.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename(model, dataset, e_idx, is_mfc=False, C=None, Ct=None):\n",
    "    base_dir = dataset\n",
    "    if is_mfc and model != 'bert':\n",
    "        base_dir = 'framing_corpus_' + dataset\n",
    "    if model in lda_pretrained:\n",
    "        base_dir += '.lda_pretrained'\n",
    "    \n",
    "    fname = model + '.result.split'+str(e_idx)\n",
    "\n",
    "    if C is not None and Ct is not None:\n",
    "        fname += '.setting=' + str((C,Ct))\n",
    "    elif C is not None:\n",
    "        fname += '.setting=' + str((C,1e-6))\n",
    "        \n",
    "    return os.path.join(out, base_dir, fname)\n",
    "        \n",
    "def create_results_table(exp_results, model_settings_dict, dataset):\n",
    "    data = []\n",
    "    for model in models:\n",
    "        if dataset in TextResponseDataset.CLASSIFICATION_SETTINGS or dataset in framing_corpus_topics:\n",
    "            metric_idx = metrics['acc']\n",
    "        else:\n",
    "            metric_idx = metrics['mse']\n",
    "            \n",
    "        values = [method_names[model]]\n",
    "        setting = model_settings_dict[model][dataset]        \n",
    "        average_error = exp_results[dataset][model][setting].mean(axis=0)[metric_idx]\n",
    "        std = exp_results[dataset][model][setting].std(axis=0)[metric_idx]\n",
    "        values += [str(np.round(average_error, 2)) + ' (' + str(np.round(std,4)) + ')']\n",
    "        data.append(values)\n",
    "    results_df = pd.DataFrame(data, columns=['Model name', 'Mean performance'])\n",
    "    return results_df\n",
    "\n",
    "def get_best_results(exp_results, data, model, is_classification=False):\n",
    "    best_score = 1e7 if not is_classification else 0.\n",
    "    best_config = None\n",
    "    metric_idx = metrics['mse'] if not is_classification else metrics['acc']\n",
    "    \n",
    "    for config, res in exp_results[data][model].items():\n",
    "        mean_results = res.mean(axis=0)\n",
    "        current_score = mean_results[metric_idx]\n",
    "        if not is_classification:\n",
    "            if current_score < best_score:\n",
    "                best_score = current_score\n",
    "                best_config = config\n",
    "        else:\n",
    "            if current_score > best_score:\n",
    "                best_score = current_score\n",
    "                best_config = config\n",
    "    return best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_results = {data:{'hstm-all':{(C,Ct):np.zeros((num_folds,n_metrics)) for C in Cs for Ct in Ctopics}}\n",
    "                     for data in datasets}\n",
    "for data in datasets:\n",
    "    exp_results[data].update({'stm+bow':{(C,1e-6):np.zeros((num_folds,n_metrics)) for C in Cs}}) \n",
    "    exp_results[data].update({'stm':{(C,1e-6):np.zeros((num_folds,n_metrics)) for C in Cs}}) \n",
    "    exp_results[data].update({'bert':{(0.,0):np.zeros((num_folds,n_metrics-3))}})\n",
    "    exp_results[data].update({'slda':{(0.,0):np.zeros((num_folds,n_metrics-1))}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in ['deathpenalty']:\n",
    "    is_framing_corpus = False\n",
    "    if data in framing_corpus_topics:\n",
    "        is_framing_corpus=True\n",
    "    \n",
    "    for model in models:\n",
    "        for e_idx in range(num_folds):\n",
    "            if model == 'bert' or model == 'slda':\n",
    "                filename = get_filename(model, data, e_idx, is_mfc=is_framing_corpus)\n",
    "                results = np.load(filename + '.npy')\n",
    "                exp_results[data][model][(0.,0.,)][e_idx]=results\n",
    "            elif model == 'stm' or model == 'stm+bow':\n",
    "                for C in Cs:\n",
    "                    filename = get_filename(model, data, e_idx, is_mfc=is_framing_corpus, C=C)\n",
    "                    results = np.load(filename + '.npy')\n",
    "                    exp_results[data][model][(C,1e-6)][e_idx]=results\n",
    "            else:\n",
    "                for (C, Ct) in it.product(Cs, Ctopics):\n",
    "                    filename = get_filename(model, data, e_idx, is_mfc=is_framing_corpus, C=C, Ct=Ct)\n",
    "                    results = np.load(filename + '.npy')\n",
    "                    exp_results[data][model][(C,Ct)][e_idx]=results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_settings_dict = {model:{data:None for data in datasets} for model in models}\n",
    "for model in models:\n",
    "    for data in ['deathpenalty']:\n",
    "        if model == 'bert' or model == 'slda':\n",
    "            model_settings_dict[model][data] = (0.,0.)\n",
    "        else:\n",
    "            is_classification = (data in TextResponseDataset.CLASSIFICATION_SETTINGS) or (data in framing_corpus_topics)\n",
    "            model_settings_dict[model][data] = get_best_results(exp_results, \n",
    "                                                                data, \n",
    "                                                                model, \n",
    "                                                                is_classification=is_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model name</th>\n",
       "      <th>Mean performance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HSTM</td>\n",
       "      <td>0.78 (0.0233)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>STM</td>\n",
       "      <td>0.69 (0.0139)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HSTM-Het</td>\n",
       "      <td>0.76 (0.0138)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sLDA</td>\n",
       "      <td>0.69 (0.0139)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BERT</td>\n",
       "      <td>0.7 (0.016)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model name Mean performance\n",
       "0       HSTM    0.78 (0.0233)\n",
       "1        STM    0.69 (0.0139)\n",
       "2   HSTM-Het    0.76 (0.0138)\n",
       "3       sLDA    0.69 (0.0139)\n",
       "4       BERT      0.7 (0.016)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = create_results_table(exp_results, model_settings_dict, 'deathpenalty')\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
